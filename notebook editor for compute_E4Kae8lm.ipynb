{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from dataikuapi.dss.ml import DSSPredictionMLTaskSettings\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these constants by your own values\n",
    "XP_TRACKING_FOLDER_ID = \"E4Kae8lm\"\n",
    "MLFLOW_EXPERIMENT_NAME = \"uci-banking-demo\"\n",
    "MLFLOW_CODE_ENV_NAME = \"mlflow-env\"\n",
    "SAVED_MODEL_NAME = \"uci-bank-clf\"\n",
    "EVALUATION_DATASET = \"uci_bank_evaluate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "def now_str() -> str:\n",
    "    return datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment tracking (scikit-learn)\n",
    "\n",
    "This notebook contains a simple example to showcase the new Experiment Tracking capabilities of Dataiku. It explains how to perform several runs with different parameters, select the best run and promote it as a Saved Model version in a Dataiku Flow. It leverages:\n",
    "* the [scikit-learn]() package\n",
    "* the [UCI Bank Marketing dataset]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the training data\n",
    "\n",
    "Our training data lives in the `uci_bank_train` Dataset, let's load it in a pandas DataFrame and see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <button style=\"display:none\" \n",
       "            class=\"btn btn-default ipython-export-btn\" \n",
       "            id=\"btn-df-43f91d13-b2db-411a-80cf-ce7b20a49a48\" \n",
       "            onclick=\"_export_df('43f91d13-b2db-411a-80cf-ce7b20a49a48')\">\n",
       "                Export dataframe\n",
       "            </button>\n",
       "            \n",
       "            <script>\n",
       "                \n",
       "                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n",
       "                    console.log('Checking dataframe exportability...')\n",
       "                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n",
       "                        console.log('Export is not possible (IPython kernel is not available)')\n",
       "                        if(no_fn) {\n",
       "                            no_fn();\n",
       "                        }\n",
       "                    } else {\n",
       "                        var pythonCode = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"'+dfid+'\")';\n",
       "                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n",
       "                            console.info(\"Exportability response\", resp);\n",
       "                            var size = /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n",
       "                            if(!size) {\n",
       "                                console.log('Export is not possible (dataframe is not in-memory anymore)')\n",
       "                                if(no_fn) {\n",
       "                                    no_fn();\n",
       "                                }\n",
       "                            } else {\n",
       "                                console.log('Export is possible')\n",
       "                                if(yes_fn) {\n",
       "                                    yes_fn(1*size[1],1*size[2]);\n",
       "                                }\n",
       "                            }\n",
       "                        }}});\n",
       "                    }\n",
       "                }\n",
       "            \n",
       "                function _export_df(dfid) {\n",
       "                    \n",
       "                    var btn = $('#btn-df-'+dfid);\n",
       "                    var btns = $('.ipython-export-btn');\n",
       "                    \n",
       "                    _check_export_df_possible(dfid,function() {\n",
       "                        \n",
       "                        window.parent.openExportModalFromIPython('Pandas dataframe',function(data) {\n",
       "                            btns.prop('disabled',true);\n",
       "                            btn.text('Exporting...');\n",
       "                            var command = 'from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"'+dfid+'\",\"'+data.exportId+'\")';\n",
       "                            var callback = {iopub:{output: function(resp) {\n",
       "                                console.info(\"CB resp:\", resp);\n",
       "                                _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                    $('#btn-df-'+dfid)\n",
       "                                        .css('display','inline-block')\n",
       "                                        .text('Export this dataframe ('+rows+' rows, '+cols+' cols)')\n",
       "                                        .prop('disabled',false);\n",
       "                                },function() {\n",
       "                                    $('#btn-df-'+dfid).css('display','none');\n",
       "                                });\n",
       "                            }}};\n",
       "                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n",
       "                        });\n",
       "                    \n",
       "                    }, function(){\n",
       "                            alert('Unable to export : the Dataframe object is not loaded in memory');\n",
       "                            btn.css('display','none');\n",
       "                    });\n",
       "                    \n",
       "                }\n",
       "                \n",
       "                (function(dfid) {\n",
       "                \n",
       "                    var retryCount = 10;\n",
       "                \n",
       "                    function is_valid_websock(s) {\n",
       "                        return s && s.readyState==1;\n",
       "                    }\n",
       "                \n",
       "                    function check_conn() {\n",
       "                        \n",
       "                        if(!IPython || !IPython.notebook) {\n",
       "                            // Don't even try to go further\n",
       "                            return;\n",
       "                        }\n",
       "                        \n",
       "                        // Check if IPython is ready\n",
       "                        console.info(\"Checking conn ...\")\n",
       "                        if(IPython.notebook.kernel\n",
       "                        && IPython.notebook.kernel\n",
       "                        && is_valid_websock(IPython.notebook.kernel.ws)\n",
       "                        ) {\n",
       "                            \n",
       "                            _check_export_df_possible(dfid,function(rows, cols) {\n",
       "                                $('#btn-df-'+dfid).css('display','inline-block');\n",
       "                                $('#btn-df-'+dfid).text('Export this dataframe ('+rows+' rows, '+cols+' cols)');\n",
       "                            });\n",
       "                            \n",
       "                        } else {\n",
       "                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n",
       "                            \n",
       "                            // Retry later\n",
       "                            \n",
       "                            if(retryCount>0) {\n",
       "                                setTimeout(check_conn,500);\n",
       "                                retryCount--;\n",
       "                            }\n",
       "                            \n",
       "                        }\n",
       "                    };\n",
       "                    \n",
       "                    setTimeout(check_conn,100);\n",
       "                    \n",
       "                })(\"43f91d13-b2db-411a-80cf-ce7b20a49a48\");\n",
       "                \n",
       "            </script>\n",
       "            \n",
       "        <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>retired</td>\n",
       "      <td>married</td>\n",
       "      <td>primary</td>\n",
       "      <td>no</td>\n",
       "      <td>121</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>593</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  contact  day month  duration  campaign  pdays  previous poutcome   y\n",
       "0   58    management  married   tertiary      no     2143     yes   no  unknown    5   may       261         1     -1         0  unknown  no\n",
       "1   44    technician   single  secondary      no       29     yes   no  unknown    5   may       151         1     -1         0  unknown  no\n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes  unknown    5   may        76         1     -1         0  unknown  no\n",
       "3   58       retired  married    primary      no      121     yes   no  unknown    5   may        50         1     -1         0  unknown  no\n",
       "4   43    technician   single  secondary      no      593     yes   no  unknown    5   may        55         1     -1         0  unknown  no"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = dataiku.api_client()\n",
    "project = client.get_default_project()\n",
    "training_dataset = dataiku.Dataset(\"UCI_BANK_TRAIN\")\n",
    "df = training_dataset.get_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working on a * binary classification* problem here, which is to predict whether or not a given person who was part of a marketing campaign ended up purchasing one of the bank's products. This outcome is reflected by the `y` column which can either take the \"no\" or \"yes\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n"
     ]
    }
   ],
   "source": [
    "target_name = \"y\"\n",
    "target = df[target_name]\n",
    "data = df.drop(columns=[target_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the experiment\n",
    "\n",
    "To prepare the grounds for our experiments, we need to create a few handles and define which MLFlow experiment we'll collect our runs into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mlflow_extension object to easily collect information for the promotion step\n",
    "mlflow_extension = project.get_mlflow_extension()\n",
    "\n",
    "# Get a handle on the Managed Folder that will contain the experiment run + artifact data\n",
    "# TODO Get-or-create managed folder\n",
    "folder = project.get_managed_folder(odb_id=XP_TRACKING_FOLDER_ID)\n",
    "\n",
    "# Create a handle for the mlflow client\n",
    "mlflow_handle = project.setup_mlflow(managed_folder=folder)\n",
    "\n",
    "# Set the experiment\n",
    "mlflow.set_experiment(experiment_name=MLFLOW_EXPERIMENT_NAME)\n",
    "mlflow_experiment = mlflow.get_experiment_by_name(MLFLOW_EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimenting\n",
    "\n",
    "The goal of experiment tracking is to *instrument the iterative process of ML model training* by collecting all parameters and results of each trial. To be more specific, within an **experiment**, you perform multiple **runs**, each run being different from the others because of the **parameters** you use for it. You also need to specific which **metrics** to track, they will reflect the performance of the model for a given set of parameters.\n",
    "\n",
    "In this notebook example, if you want to produce experiment runs:\n",
    "* edit the parameters in the 3.1 cell and run it\n",
    "* run the 3.2 cell to effectively... perform the run ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Defining the parameters of our run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters to log:\n",
      " {'categorical_cols': ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'], 'model_algo': 'RandomForestClassifier', 'n_estimators': 100, 'criterion': 'gini', 'max_depth': 6, 'min_samples_split': 5, 'random_state': 42, 'n_cv_folds': 3}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Metrics to log:\n",
      " ['f1_macro', 'roc_auc']\n"
     ]
    }
   ],
   "source": [
    "# Create run name\n",
    "run_name = f\"run-{now_str()}\"\n",
    "run_params = {}\n",
    "run_metrics = {}\n",
    "\n",
    "# Define run parameters\n",
    "# -- Which categorical columns to retain ?\n",
    "categorical_cols = [\"job\",\n",
    "                    \"marital\",\n",
    "                    \"education\",\n",
    "                    \"default\",\n",
    "                    \"housing\",\n",
    "                    \"loan\",\n",
    "                    \"contact\",\n",
    "                    \"month\",\n",
    "                    \"poutcome\"]\n",
    "run_params[\"categorical_cols\"] = categorical_cols\n",
    "\n",
    "# --Which algorithm to use? Which hyperparameters for this algo to try?\n",
    "# --- Example: Random Forest\n",
    "hparams = {\"n_estimators\": 100,\n",
    "           \"criterion\": \"gini\",\n",
    "           \"max_depth\": 6,\n",
    "           \"min_samples_split\": 5,\n",
    "           \"random_state\": 42}\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\"\"\"\n",
    "grid_rf_model = GridSearchCV(rf, parameters, cv=3)\n",
    "grid_rf_model.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid_rf_model.best_estimator_\n",
    "for p in parameters:\n",
    "  print(\"Best '{}': {}\".format(p, best_rf.get_params()[p]))\n",
    "\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "clf = GridSearchCV(rf, hparams, cv=3)\n",
    "\"\"\"\n",
    "\n",
    "# ---Example: Gradient Boosting\n",
    "#hparams = {\"n_estimators\": 300,\n",
    "#           \"loss\": \"exponential\",\n",
    "#           \"learning_rate\": 0.1,\n",
    "#           \"max_depth\": 3,\n",
    "#           \"random_state\": 42}\n",
    "#clf = GradientBoostingClassifier(**hparams)\n",
    "model_algo = type(clf).__name__\n",
    "run_params[\"model_algo\"] = model_algo\n",
    "for hp in hparams.keys():\n",
    "    run_params[hp] = hparams[hp]\n",
    "\n",
    "# --Which cross-validation settings to use?\n",
    "n_cv_folds = 3\n",
    "cv = StratifiedKFold(n_splits=n_cv_folds)\n",
    "run_params[\"n_cv_folds\"] = n_cv_folds\n",
    "metrics = [\"f1_macro\", \"roc_auc\"]\n",
    "\n",
    "# --Let's print all of that to get a recap:\n",
    "print(f\"Parameters to log:\\n {run_params}\")\n",
    "print(100*'-')\n",
    "print(f\"Metrics to log:\\n {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Performing the run and logging parameters, metrics and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run run-20230224005222 (id: run_20230224005222)...\n",
      "Running cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n",
      "/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/pandas/core/indexes/base.py:395: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  elif issubclass(data.dtype.type, np.bool) or is_bool_dtype(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE! Your artifacts are available at dss-managed-folder://E4Kae8lm/uci_banking_demo/run_20230224005222/artifacts\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"Starting run {run_name} (id: {run_id})...\")\n",
    "    # --Preprocessing\n",
    "    categorical_preprocessor = OrdinalEncoder()\n",
    "    preprocessor = ColumnTransformer([('categorical', categorical_preprocessor, categorical_cols)],\n",
    "                                     remainder=\"passthrough\")\n",
    "\n",
    "    # --Pipeline definition (preprocessing + model)\n",
    "    pipeline = make_pipeline(preprocessor, clf)\n",
    "\n",
    "    # --Cross-validation\n",
    "    print(f\"Running cross-validation...\")\n",
    "    scores = cross_validate(pipeline, data, target, cv=cv, scoring=metrics)\n",
    "    for m in [f\"test_{mname}\" for mname in metrics]:\n",
    "        run_metrics[f\"mean_{m}\"] = scores[m].mean()\n",
    "        run_metrics[f\"std_{m}\"] = scores[m].std()\n",
    "\n",
    "    # --Pipeline fit\n",
    "    pipeline.fit(X=data, y=target)\n",
    "    # --Log the order of the class label\n",
    "    run_params[\"class_labels\"] = pipeline.classes_.tolist()\n",
    "\n",
    "    # --Log parameters, metrics and model\n",
    "    mlflow.log_params(params=run_params)\n",
    "    mlflow.log_metrics(metrics=run_metrics)\n",
    "    artifact_path = f\"{model_algo}-{run_id}\"\n",
    "    mlflow.sklearn.log_model(sk_model=pipeline, artifact_path=artifact_path)\n",
    "\n",
    "    # --Set useful information to faciliate run promotion\n",
    "    mlflow_extension.set_run_inference_info(run_id=run_id,\n",
    "                                            prediction_type=\"BINARY_CLASSIFICATION\",\n",
    "                                            classes=run_params[\"class_labels\"],\n",
    "                                            code_env_name=MLFLOW_EXPERIMENT_NAME,\n",
    "                                            target=\"y\")\n",
    "    print(f\"DONE! Your artifacts are available at {run.info.artifact_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Promote a run to a Saved Model version\n",
    "\n",
    "Now that you have tried several parameters and performed multiple runs, you may want to choose the \"best\" one and actually surface it on your Dataiku Flow. This is done by *promoting* an experiment run into a Saved Model version. Let's start by figuring out which experiment got the best results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all runs of the experiment with performance metrics\n",
    "run_cv_metrics = {}\n",
    "for run_info in mlflow.list_run_infos(experiment_id=mlflow_experiment.experiment_id):\n",
    "    run = mlflow.get_run(run_info.run_id)\n",
    "    run_cv_metrics[run_info.run_id] = run.data.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best run is run_20230224004210 with a mean_test_roc_auc of 0.5403114819487171\n"
     ]
    }
   ],
   "source": [
    "# Look for the best run according to a given metric:\n",
    "metric_for_promotion = \"mean_test_roc_auc\"\n",
    "# --Only keep the runs where the metric for promotion was logged:\n",
    "simplified_run_cv_metrics = {}\n",
    "for rid in run_cv_metrics.keys():\n",
    "    if metric_for_promotion in run_cv_metrics[rid].keys():\n",
    "        simplified_run_cv_metrics[rid] = run_cv_metrics[rid][metric_for_promotion]\n",
    "best_run_id = sorted(simplified_run_cv_metrics)[0]\n",
    "print(f\"The best run is {best_run_id} with a {metric_for_promotion} of {simplified_run_cv_metrics[best_run_id]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_20230224005222': 0.5278375524347928,\n",
       " 'run_20230224004210': 0.5403114819487171}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplified_run_cv_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its model is located at: uci_banking_demo/run_20230224004210/artifacts/RandomForestClassifier-run_20230224004210\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the path of its model directory\n",
    "best_run_info = mlflow.get_run(best_run_id).info\n",
    "model_path = f\"{mlflow_experiment.experiment_id}/{best_run_id}/artifacts/{model_algo}-{best_run_id}\"\n",
    "print(f\"Its model is located at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RunInfo: artifact_uri='dss-managed-folder://E4Kae8lm/uci_banking_demo/run_20230224004210/artifacts', end_time=1677199348423, experiment_id='uci_banking_demo', lifecycle_stage='active', run_id='run_20230224004210', run_uuid='run_20230224004210', start_time=1677199332644, status='FINISHED', user_id='dssuser_kirsten_da4e5f91'>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dss-managed-folder://E4Kae8lm/uci_banking_demo/run_20230224004210/artifacts'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.info.artifact_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's retrieve a handle for the Saved Model in which we are going to create our version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Saved Model uci-bank-clf with id nvVYOuGW\n"
     ]
    }
   ],
   "source": [
    "# Get or create the Saved Model\n",
    "sm_id = None\n",
    "for sm in project.list_saved_models():\n",
    "    if sm[\"name\"] != SAVED_MODEL_NAME:\n",
    "        continue\n",
    "    else:\n",
    "        sm_id = sm[\"id\"]\n",
    "        print(f\"Found Saved Model {sm['name']} with id {sm['id']}\")\n",
    "        break\n",
    "if sm_id:\n",
    "    sm = project.get_saved_model(sm_id)\n",
    "else:\n",
    "    sm = project.create_mlflow_pyfunc_model(name=SAVED_MODEL_NAME,\n",
    "                                            prediction_type=DSSPredictionMLTaskSettings.PredictionTypes.BINARY)\n",
    "    sm_id = sm.id\n",
    "    print(f\"Saved Model not found, created new one with id {sm_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now promote our experiment run and generate the corresponding Saved Model version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import model by generating a Saved Model version\n",
    "version_id = f\"{best_run_id}_{now_str()}\"\n",
    "mlflow_version = sm.import_mlflow_version_from_managed_folder(version_id=version_id,\n",
    "                                                              managed_folder=XP_TRACKING_FOLDER_ID,\n",
    "                                                              path=model_path)\n",
    "# Make this Saved Model version the active one\n",
    "sm.set_active_version(mlflow_version.version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to be able to visualize the performance graphs of our newly-created Saved Model version, we need to *evaluate* it against an evaluation Dataset. In our case, it's called `uci_bank_evaluate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "DataikuException",
     "evalue": "com.dataiku.dip.server.controllers.NotFoundException: dataset does not exist: EMERSONEVAL.uci_bank_evaluate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/dataiku-dss-11.3.1/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body, headers)\u001b[0m\n\u001b[1;32m   1322\u001b[0m                     headers=headers)\n\u001b[0;32m-> 1323\u001b[0;31m             \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/dataiku/dss_data/code-envs/python/Python38/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://ip-10-0-1-219.ec2.internal:10001/dip/publicapi/projects/EMERSONEVAL/savedmodels/nvVYOuGW/versions/run_20230224004210_20230224005407/external-ml/metadata?containerExecConfigName=NONE",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDataikuException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a3ec4c3a0a4e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the imported model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m mlflow_version.set_core_metadata(target_column_name=target_name,\n\u001b[0m\u001b[1;32m      3\u001b[0m                                  \u001b[0mclass_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# TODO change with run params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  get_features_from_dataset=EVALUATION_DATASET)\n\u001b[1;32m      5\u001b[0m \u001b[0mmlflow_version\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEVALUATION_DATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/dataiku-dss-11.3.1/python/dataikuapi/dss/savedmodel.py\u001b[0m in \u001b[0;36mset_core_metadata\u001b[0;34m(self, target_column_name, class_labels, get_features_from_dataset, features_list, output_style, container_exec_config_name)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         self.saved_model.client._perform_empty(\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \"/projects/{project_key}/savedmodels/{sm_id}/versions/{version_id}/external-ml/metadata?containerExecConfigName={container_exec_config_name}\".format(\n",
      "\u001b[0;32m/opt/dataiku-dss-11.3.1/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_empty\u001b[0;34m(self, method, path, params, body, files, raw_body)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_perform_http\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/dataiku-dss-11.3.1/python/dataikuapi/dssclient.py\u001b[0m in \u001b[0;36m_perform_http\u001b[0;34m(self, method, path, params, body, stream, files, raw_body, headers)\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttp_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDataikuException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errorType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unknown error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_perform_empty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataikuException\u001b[0m: com.dataiku.dip.server.controllers.NotFoundException: dataset does not exist: EMERSONEVAL.uci_bank_evaluate"
     ]
    }
   ],
   "source": [
    "# Evaluate the imported model\n",
    "mlflow_version.set_core_metadata(target_column_name=target_name,\n",
    "                                 class_labels=[\"no\", \"yes\"], # TODO change with run params\n",
    "                                 get_features_from_dataset=EVALUATION_DATASET)\n",
    "mlflow_version.evaluate(EVALUATION_DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now have a Saved Model version coming from a fully programmatic workflow!"
   ]
  }
 ],
 "metadata": {
  "associatedRecipe": "compute_E4Kae8lm",
  "createdOn": 1677199006897,
  "creator": "kirsten.hoogenakker",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env Python38)",
   "language": "python",
   "name": "py-dku-venv-python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "modifiedBy": "kirsten.hoogenakker",
  "tags": [
   "recipe-editor"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
